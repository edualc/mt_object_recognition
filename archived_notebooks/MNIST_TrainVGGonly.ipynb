{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4de7608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef transform_pillow_to_torch(x):\\n    #x = x.resize((224,224), resample=pillow.Image.BILINEAR).convert('RGB')\\n    #return torchvision.transforms.ToTensor()(x)\\n    return torchvision.transforms.ToTensor()(torchvision.transforms.Grayscale(num_output_channels=3)(x))\\n\\ndef plt_t(x):\\n    plt.imshow(torch.swapaxes(x,0,2))\\n\\nmnist = MNIST('images/mnist/', download=True, transform=transform_pillow_to_torch)\\nomniglot = Omniglot('images/omniglot/', download=True, transform=transform_pillow_to_torch)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as M\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, rand_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython import display\n",
    "\n",
    "from lateral_connections import LateralModel, VggModel, CustomImageDataset\n",
    "\n",
    "import PIL as pillow\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST, Omniglot\n",
    "\n",
    "\"\"\"\n",
    "def transform_pillow_to_torch(x):\n",
    "    #x = x.resize((224,224), resample=pillow.Image.BILINEAR).convert('RGB')\n",
    "    #return torchvision.transforms.ToTensor()(x)\n",
    "    return torchvision.transforms.ToTensor()(torchvision.transforms.Grayscale(num_output_channels=3)(x))\n",
    "\n",
    "def plt_t(x):\n",
    "    plt.imshow(torch.swapaxes(x,0,2))\n",
    "\n",
    "mnist = MNIST('images/mnist/', download=True, transform=transform_pillow_to_torch)\n",
    "omniglot = Omniglot('images/omniglot/', download=True, transform=transform_pillow_to_torch)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01983f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: images/mnist/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: <function transform_data at 0x7f4319628280>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f43195f9e80>\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "dataset = 'mnist'\n",
    "\n",
    "def transform_data(x):\n",
    "    x = torchvision.transforms.ToTensor()(x)\n",
    "    return x\n",
    "\n",
    "def transform_corrupt(x):\n",
    "    x = torchvision.transforms.ToTensor()(x)\n",
    "    x[:, :x.shape[1]//2, :] = torch.median(x)\n",
    "    return x\n",
    "\n",
    "if dataset == 'mnist':\n",
    "    num_classes = 10\n",
    "    train_data = torchvision.datasets.MNIST('images/mnist/', train=True, transform=transform_data, download=True)\n",
    "    test_data = torchvision.datasets.MNIST('images/mnist/', train=False, transform=transform_corrupt, download=True)\n",
    "\n",
    "elif dataset == 'omniglot':\n",
    "    num_classes = 964\n",
    "    train_data = torchvision.datasets.Omniglot('images/mnist/', background=True, transform=transform_data, download=True)\n",
    "    test_data = torchvision.datasets.Omniglot('images/mnist/', background=False, transform=transform_corrupt, download=True)\n",
    "    \n",
    "print(train_data)\n",
    "    \n",
    "loaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True, num_workers=1),\n",
    "    'test': torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True, num_workers=1)\n",
    "}\n",
    "\n",
    "print(loaders['train'])\n",
    "print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "503d43f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f43195f9ee0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMQ0lEQVR4nO3df6jddR3H8dcrrX9s4JZ0HdtqGaIMkRlzBkkUsjD/cG5gJBKLotsfLgv6wx/9kaBCZj/WPyYbym5RRqC2EdGyqa0QhldZOjdKi0lerlsikomQu3v3x/luXOc533P3/XG+Z3s/H3A453zf3+/5vvmy176/zrkfR4QAnPne13UDAEaDsANJEHYgCcIOJEHYgSTOHuXKbHPpH2hZRLjf9Fp7dttX2/6b7Zds31rnswC0y1Xvs9s+S9LfJa2T9IqkpyXdEBEHSpZhzw60rI09+1pJL0XEPyPif5J+JWl9jc8D0KI6YV8m6V/z3r9STHsX25O2p21P11gXgJpav0AXEVslbZU4jAe6VGfPPiNpxbz3y4tpAMZQnbA/LelC2x+z/QFJX5S0s5m2ADSt8mF8RBy1vVnSLklnSXowIl5orDMAjap8663SyjhnB1rXypdqAJw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJVB6fXZJsH5L0pqQ5SUcjYk0TTQFoXq2wFz4bEa818DkAWsRhPJBE3bCHpD/Yfsb2ZL8ZbE/anrY9XXNdAGpwRFRf2F4WETO2PyzpMUnfiIg9JfNXXxmABYkI95tea88eETPF8xFJj0paW+fzALSncthtn2N70fHXkj4naX9TjQFoVp2r8ROSHrV9/HN+GRG/b6QrAI2rdc5+yivjnB1oXSvn7ABOH4QdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJIaG3faDto/Y3j9v2hLbj9l+sXhe3G6bAOpayJ59u6SrT5p2q6TdEXGhpN3FewBjbGjYI2KPpNdPmrxe0lTxekrSdc22BaBpZ1dcbiIiZovXr0qaGDSj7UlJkxXXA6AhVcN+QkSE7Sipb5W0VZLK5gPQrqpX4w/bXipJxfOR5loC0IaqYd8paVPxepOkHc20A6Atjig/srb9kKTPSDpP0mFJ35X0G0m/lvQRSS9L+kJEnHwRr99ncRgPtCwi3G/60LA3ibAD7RsUdr5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAStUeEweltw4YNpfWpqanS+qJFiyqv+7bbbqu17tnZ2dI63o09O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwSiuKHXJJZeU1rds2VJav+qqqyqv+/777y+t33XXXaX1mZmZyus+nVUexdX2g7aP2N4/b9odtmds7yse1zTZLIDmLeQwfrukq/tM/3FErC4ev2u2LQBNGxr2iNgj6fUR9AKgRXUu0G22/VxxmL940Ey2J21P256usS4ANVUN+08lfVzSakmzkn44aMaI2BoRayJiTcV1AWhApbBHxOGImIuIY5K2SVrbbFsAmlYp7LaXznu7QdL+QfMCGA9D77PbfkjSZySdJ+mwpO8W71dLCkmHJH09Iob+uJj77Geec889t7R+7bXXDqxt3769dFm77+3iEx5//PHSep17/KezQffZh/7xioi4oc/kB2p3BGCk+LoskARhB5Ig7EAShB1IgrADSfATV3TmnXfeKa2ffXb5zaKjR4+W1tetWzew9uSTT5Yuezqr/BNXAGcGwg4kQdiBJAg7kARhB5Ig7EAShB1IgiGbUerSSy8trV9//fWl9bVrB/9dk2H30Yc5cOBAaX3Pnj21Pv9Mw54dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5LgPvsZ7qKLLiqt33zzzaX1jRs3ltbPP//8U+5poebm5krrs7Plf7382LFjTbZz2mPPDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcJ/9NDDsXvaNN944sLZ58+bSZVeuXFmlpUZMT0+X1u+8887S+s6dO5ts54w3dM9ue4XtJ2wfsP2C7W8W05fYfsz2i8Xz4vbbBVDVQg7jj0r6dkSskvRJSTfZXiXpVkm7I+JCSbuL9wDG1NCwR8RsRDxbvH5T0kFJyyStlzRVzDYl6bqWegTQgFM6Z7e9UtJlkvZKmoiI419OflXSxIBlJiVN1ugRQAMWfDXe9gclPSzpWxHxn/m16I0O2XfQxojYGhFrImJNrU4B1LKgsNt+v3pB/0VEPFJMPmx7aVFfKulIOy0CaMLQw3jblvSApIMR8aN5pZ2SNkn6XvG8o5UOzwATE33PcE5YtWpVaf2+++4rrV988cWn3FNT9u7dW1q/5557BtZ27Cj/J8NPVJu1kHP2T0n6kqTnbe8rpt2uXsh/bfurkl6W9IVWOgTQiKFhj4i/SOo7uLukq5ptB0Bb+LoskARhB5Ig7EAShB1IgrADSfAT1wVasmTJwNq2bdtKl129enVp/YILLqjSUiOeeuqp0vq9995bWt+1a1dp/e233z7lntAO9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kESa++xXXHFFaf2WW24prV9++eUDa8uXL6/UU1PK7mVv2bKldNm77767tP7WW29VaQljiD07kARhB5Ig7EAShB1IgrADSRB2IAnCDiSR5j77xo0bS+sbNmxobd0HDx4srQ8benhubq60Xvab8zfeeKN0WeTBnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHknBElM9gr5D0M0kTkkLS1oj4ie07JH1N0r+LWW+PiN8N+azylQGoLSL6jrq8kLAvlbQ0Ip61vUjSM5KuU2889v9GxA8W2gRhB9o3KOwLGZ99VtJs8fpN2wclLWu2PQBtO6VzdtsrJV0maW8xabPt52w/aHvxgGUmbU/bnq7XKoA6hh7Gn5jR/qCkP0m6OyIesT0h6TX1zuPvVO9Q/ytDPoPDeKBllc/ZJcn2+yX9VtKuiPhRn/pKSb+NiEuGfA5hB1o2KOxDD+NtW9IDkg7OD3px4e64DZL2120SQHsWcjX+Skl/lvS8pGPF5Nsl3SBptXqH8Yckfb24mFf2WezZgZbVOoxvCmEH2lf5MB7AmYGwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxKiHbH5N0svz3p9XTBtH49rbuPYl0VtVTfb20UGFkf6e/T0rt6cjYk1nDZQY197GtS+J3qoaVW8cxgNJEHYgia7DvrXj9ZcZ197GtS+J3qoaSW+dnrMDGJ2u9+wARoSwA0l0EnbbV9v+m+2XbN/aRQ+D2D5k+3nb+7oen64YQ++I7f3zpi2x/ZjtF4vnvmPsddTbHbZnim23z/Y1HfW2wvYTtg/YfsH2N4vpnW67kr5Gst1Gfs5u+yxJf5e0TtIrkp6WdENEHBhpIwPYPiRpTUR0/gUM25+W9F9JPzs+tJbt70t6PSK+V/xHuTgibhmT3u7QKQ7j3VJvg4YZ/7I63HZNDn9eRRd79rWSXoqIf0bE/yT9StL6DvoYexGxR9LrJ01eL2mqeD2l3j+WkRvQ21iIiNmIeLZ4/aak48OMd7rtSvoaiS7CvkzSv+a9f0XjNd57SPqD7WdsT3bdTB8T84bZelXSRJfN9DF0GO9ROmmY8bHZdlWGP6+LC3TvdWVEfELS5yXdVByujqXonYON073Tn0r6uHpjAM5K+mGXzRTDjD8s6VsR8Z/5tS63XZ++RrLdugj7jKQV894vL6aNhYiYKZ6PSHpUvdOOcXL4+Ai6xfORjvs5ISIOR8RcRByTtE0dbrtimPGHJf0iIh4pJne+7fr1Nart1kXYn5Z0oe2P2f6ApC9K2tlBH+9h+5ziwolsnyPpcxq/oah3StpUvN4kaUeHvbzLuAzjPWiYcXW87Tof/jwiRv6QdI16V+T/Iek7XfQwoK8LJP21eLzQdW+SHlLvsO4d9a5tfFXShyTtlvSipD9KWjJGvf1cvaG9n1MvWEs76u1K9Q7Rn5O0r3hc0/W2K+lrJNuNr8sCSXCBDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+D/kreVFWS+BJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = train_data[0]\n",
    "plt.imshow(img[0,...], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "img2 = torch.clone(img)\n",
    "img2[:, :img2.shape[1]//2, :] = torch.median(img2)\n",
    "plt.imshow(img2[0,...], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6aca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        \n",
    "        # TODO: Fix for datasets\n",
    "        if num_classes == 10:\n",
    "            self.out = nn.Linear(32 * 7 * 7, num_classes)\n",
    "        else:\n",
    "            self.out = nn.Linear(32 * 26 * 26, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization\n",
    "    \n",
    "model = SimpleCNN(num_classes)\n",
    "loss_func = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)  \n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab275d3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6203c04af1f94f7f8cc590b9460abf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\tLoss: 0.0661\tAccuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccd99138ca849ae8b82b32f00735972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2\tLoss: 0.0039\tAccuracy: 0.9789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6af9553bf6443183cc23bd32641b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3\tLoss: 0.0028\tAccuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26a02152c6e422ab946bd8553d98161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4\tLoss: 0.0024\tAccuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368aa81f869749dea25ae226c9628e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5\tLoss: 0.0025\tAccuracy: 0.9840\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "def train(num_epochs, model, loaders, loss_func):\n",
    "    model.train()\n",
    "    \n",
    "    total_step = len(loaders['train'])\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), leave=False):\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        \n",
    "        for i, (images, labels) in tqdm(enumerate(loaders['train'], 0), total=len(loaders['train'])):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(images)[0]\n",
    "            \n",
    "            pred_y = torch.max(output, 1)[1].data.squeeze()             \n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += (loss.item() / total)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1:2d}\\tLoss: {round(total_loss,4):1.4f}\\tAccuracy: {round(correct/total,4):1.4f}\")\n",
    "            \n",
    "            \n",
    "train(num_epochs, model, loaders, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0359d88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb495405d9a84f12bfc2b50e0d8c1896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy is  0.9872\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2d2acaab67417abdfd3bcab84a5864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is  0.3924\n"
     ]
    }
   ],
   "source": [
    "def test(eval_mode='test'):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(loaders[eval_mode]):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            test_output, last_layer = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            \n",
    "        accuracy = correct / total\n",
    "        if eval_mode == 'test':\n",
    "            print('Test Accuracy is ', round(accuracy,4))\n",
    "        elif eval_mode == 'train':\n",
    "            print('Train Accuracy is ', round(accuracy,4))\n",
    "        \n",
    "test('train')\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
